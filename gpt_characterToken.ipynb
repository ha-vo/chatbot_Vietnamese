{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path_DataTrain = \"/content/drive/My Drive/cleanData_train.txt\"\n",
        "path_DataVal = \"/content/drive/My Drive/cleanData_val.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bzrh9JH0S--",
        "outputId": "3e06da2a-038f-4eea-b63e-d23d12033dbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intab_l = \"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ\"\n",
        "intab_u = intab_l.upper()\n",
        "ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'\n",
        "ascii_uppercase = ascii_lowercase.upper()\n",
        "digits = '0123456789'\n",
        "punctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",
        "whitespace = ' '\n",
        "accept_strings =  intab_l + intab_u + ascii_lowercase + ascii_uppercase + digits + punctuation + whitespace\n",
        "print(len(accept_strings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPD0tItY0WaX",
        "outputId": "b94ef73b-54b6-4ee8-903d-06a402e0a944"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "clLVq6Y61DzD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(accept_strings)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "with open(path_DataTrain,\"r\", encoding=\"utf-8\") as f:\n",
        "    dataTrain = f.read()\n",
        "with open(path_DataVal,\"r\", encoding=\"utf-8\") as f:\n",
        "    dataVal = f.read()\n",
        "\n",
        "# Train and test splits\n",
        "dataTrainEncoding = torch.tensor(encode(dataTrain), dtype=torch.long)\n",
        "dataValEncoding = torch.tensor(encode(dataVal), dtype=torch.long)"
      ],
      "metadata": {
        "id": "E1dYW9bS049w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "ntE24CV51M7Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = dataTrainEncoding if split == 'train' else dataValEncoding\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "9naflMfS12tV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train','val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "UFCMtXYk15U0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        # (q * k^T)/(C^(1/2)) C is dim of k\n",
        "        wei = q  @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "iJeYtnsp1_Ky"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "5BhnTG8V2Cf3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 *n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "jH8RPVX12Ffn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "if0ewVZU2IlB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emd = self.token_embedding_table(idx)\n",
        "        pos_emd = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emd + pos_emd\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "BR7v0QaJ2Kez"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters()), \"parameters\")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYucnd332Mgo",
        "outputId": "7abe5b5a-45d7-47f6-a2b7-5cf7fb1ec8ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230885 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 150000\n",
        "eval_interval = 2000\n",
        "steps_Train = {}\n",
        "steps_Val = {}\n",
        "\n",
        "for i in range(max_iters):\n",
        "    if i % eval_interval == 0 or i == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        steps_Train[str(i)] = losses['train']\n",
        "        steps_Val[str(i)] = losses['val']\n",
        "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    x , y = get_batch('train')\n",
        "    xb, yb = x,y\n",
        "    logits, loss = model(xb,yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YefphZ6f2OkB",
        "outputId": "2384ea89-2c74-49de-e2f8-68598c88b259"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 5.5415, val loss 5.5462\n",
            "step 2000: train loss 1.7356, val loss 1.7400\n",
            "step 4000: train loss 1.6169, val loss 1.6249\n",
            "step 6000: train loss 1.5608, val loss 1.5759\n",
            "step 8000: train loss 1.5302, val loss 1.5303\n",
            "step 10000: train loss 1.5002, val loss 1.5017\n",
            "step 12000: train loss 1.4777, val loss 1.4822\n",
            "step 14000: train loss 1.4620, val loss 1.4635\n",
            "step 16000: train loss 1.4376, val loss 1.4572\n",
            "step 18000: train loss 1.4170, val loss 1.4547\n",
            "step 20000: train loss 1.4254, val loss 1.4241\n",
            "step 22000: train loss 1.4036, val loss 1.4140\n",
            "step 24000: train loss 1.3921, val loss 1.4149\n",
            "step 26000: train loss 1.3904, val loss 1.4169\n",
            "step 28000: train loss 1.3730, val loss 1.3943\n",
            "step 30000: train loss 1.3747, val loss 1.3902\n",
            "step 32000: train loss 1.3691, val loss 1.3796\n",
            "step 34000: train loss 1.3518, val loss 1.3741\n",
            "step 36000: train loss 1.3624, val loss 1.3872\n",
            "step 38000: train loss 1.3637, val loss 1.3819\n",
            "step 40000: train loss 1.3564, val loss 1.3782\n",
            "step 42000: train loss 1.3495, val loss 1.3766\n",
            "step 44000: train loss 1.3447, val loss 1.3791\n",
            "step 46000: train loss 1.3361, val loss 1.3664\n",
            "step 48000: train loss 1.3294, val loss 1.3644\n",
            "step 50000: train loss 1.3328, val loss 1.3707\n",
            "step 52000: train loss 1.3330, val loss 1.3530\n",
            "step 54000: train loss 1.3204, val loss 1.3481\n",
            "step 56000: train loss 1.3295, val loss 1.3629\n",
            "step 58000: train loss 1.3102, val loss 1.3427\n",
            "step 60000: train loss 1.3113, val loss 1.3537\n",
            "step 62000: train loss 1.3110, val loss 1.3497\n",
            "step 64000: train loss 1.3185, val loss 1.3419\n",
            "step 66000: train loss 1.3044, val loss 1.3347\n",
            "step 68000: train loss 1.3199, val loss 1.3453\n",
            "step 70000: train loss 1.3175, val loss 1.3378\n",
            "step 72000: train loss 1.3133, val loss 1.3358\n",
            "step 74000: train loss 1.3040, val loss 1.3217\n",
            "step 76000: train loss 1.2834, val loss 1.3292\n",
            "step 78000: train loss 1.2987, val loss 1.3252\n",
            "step 80000: train loss 1.2917, val loss 1.3393\n",
            "step 82000: train loss 1.2990, val loss 1.3130\n",
            "step 84000: train loss 1.2838, val loss 1.3242\n",
            "step 86000: train loss 1.2894, val loss 1.3056\n",
            "step 88000: train loss 1.2827, val loss 1.3292\n",
            "step 90000: train loss 1.2838, val loss 1.3140\n",
            "step 92000: train loss 1.2839, val loss 1.3197\n",
            "step 94000: train loss 1.2825, val loss 1.3194\n",
            "step 96000: train loss 1.2843, val loss 1.3229\n",
            "step 98000: train loss 1.2975, val loss 1.3159\n",
            "step 100000: train loss 1.2794, val loss 1.3161\n",
            "step 102000: train loss 1.2853, val loss 1.3141\n",
            "step 104000: train loss 1.2867, val loss 1.3142\n",
            "step 106000: train loss 1.2838, val loss 1.3162\n",
            "step 108000: train loss 1.2821, val loss 1.3083\n",
            "step 110000: train loss 1.2730, val loss 1.3090\n",
            "step 112000: train loss 1.2896, val loss 1.3068\n",
            "step 114000: train loss 1.2763, val loss 1.3034\n",
            "step 116000: train loss 1.2742, val loss 1.3052\n",
            "step 118000: train loss 1.2804, val loss 1.3078\n",
            "step 120000: train loss 1.2716, val loss 1.2993\n",
            "step 122000: train loss 1.2755, val loss 1.3110\n",
            "step 124000: train loss 1.2786, val loss 1.2982\n",
            "step 126000: train loss 1.2767, val loss 1.3085\n",
            "step 128000: train loss 1.2715, val loss 1.3036\n",
            "step 130000: train loss 1.2713, val loss 1.3083\n",
            "step 132000: train loss 1.2732, val loss 1.2992\n",
            "step 134000: train loss 1.2691, val loss 1.3028\n",
            "step 136000: train loss 1.2739, val loss 1.3028\n",
            "step 138000: train loss 1.2759, val loss 1.3114\n",
            "step 140000: train loss 1.2668, val loss 1.3069\n",
            "step 142000: train loss 1.2622, val loss 1.3016\n",
            "step 144000: train loss 1.2588, val loss 1.3135\n",
            "step 146000: train loss 1.2616, val loss 1.3002\n",
            "step 148000: train loss 1.2669, val loss 1.3131\n",
            "step 149999: train loss 1.2678, val loss 1.3098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(m, \"/content/drive/My Drive/gptModel.pt\")"
      ],
      "metadata": {
        "id": "6ki-qh7-FO6M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/train_loss.obj\",\"wb\") as f:\n",
        "  pickle.dump(steps_Train, f)\n",
        "with open(\"/content/drive/My Drive/val_loss.obj\", \"wb\") as f:\n",
        "  pickle.dump(steps_Val, f)"
      ],
      "metadata": {
        "id": "2HyWBZ_dFXCF"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}